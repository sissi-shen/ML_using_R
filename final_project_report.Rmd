---
title: "PSTAT131_final_project"
author: "Sissi Shen"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Introduction

The purpose of this project is to build a machine learning model that can predict how well a student can adapt online education.

### Why is this model important?
Ever since the outbreak of COVID-19, institutions around the world have started adopting online teaching. However, not every student can well adopt this new form of education. Therefore, it's crucial for schools and educators to make adjustments to their form of teaching based on the overall adaptability of their students. One way of achieving this is through building a machine learning model that can predict how well the students can perform in online education based on their characteristics. 

### Table of Contents
This report starts with an Exploratory Data Analysis (EDA) section that tidies raw data and explores whether there are any patterns among the predictor variables. \

It then moves on to discussing data splitting and cross-validation. This is an important step for the actual model-building that comes later.\

The third part includes the fittings of various models. The result of each model is analyzed and discussed in the model selection and performance section. \

In the end, after choosing the best model for our data, it concludes the report by interpreting the outcomes of the models it fits. 


### Loading data and packages
Loading the packages:\
```{r}
library(tidyverse)
library(ggplot2)
library(tidymodels)
library(tidyr)
library(dplyr)
library(plyr)
library(ggpubr)
library(janitor)
library(glmnet)
library(rpart.plot)
library(vip)
library(randomForest)
library(xgboost)
library(discrim)
library(poissonreg)
library(corrr)
library(klaR)
library(kknn)
tidymodels_prefer()
```

Loading the data set:\
```{r}
adapt <- read.csv("students_adaptability.csv")
```

## Exploratory Data Analysis

Here in this section, we are going to explore raw data by tidying it to build predictive models later and perform some initial investigations on it to discover patterns or to spot any anomalies.\

### Data Cleaning
Before exploring the data set, let's first to do some data cleaning:\
  · Clean names\
```{r}
adapt <- adapt %>%
  clean_names()
```
 
  · Change the variables to factors: (since all of our variables are categorical, we need to convert all of them to factors)\
```{r}
adapt <- adapt %>%
  dplyr::mutate(gender = factor(gender, levels = c("Boy", "Girl")), 
         age = factor(age, levels = c("1-5", "6-10", "11-15", "16-20", "21-25", "26-30")), 
         education_level = factor(education_level, levels = c("School", "College", "University")),
         institution_type = factor(institution_type, levels = c("Government", "Non Government")),
         it_student = factor(it_student, levels = c("Yes", "No")), 
         location = factor(location, levels = c("Yes", "No")), 
         load_shedding = factor(load_shedding, levels = c("Low", "High")), 
         financial_condition = factor(financial_condition, levels = c("Poor", "Mid", "Rich")), 
         internet_type = factor(internet_type, levels = c("Mobile Data", "Wifi")), 
         network_type = factor(network_type, levels = c("2G", "3G", "4G")), 
         class_duration = factor(class_duration, levels = c("0", "1-3", "3-6")), 
         self_lms = factor(self_lms, levels = c("Yes", "No")), 
         device = factor(device, levels = c("Mobile", "Computer", "Tab")), 
         adaptivity_level = factor(adaptivity_level, levels = c("Low", "Moderate", "High")))
```

Now we've finished cleaning raw data, it's time to do some exploratory data analysis!\

### Data Visualizations and Analysis

#### Summary table
First, let's take a look at the summary of our data set:\

```{r}
adapt %>%
  summary()
```
There are several things that we need to pay attention to: 
  (1) The number of boys involved in data collection is more than the number of girls, so this might affect our gender analysis. 
  (2) We have much more data for students aged between 11-25 comparing to younger kids and older students, so our models later might not do good job predicting the adaptivity level for students whose age are between 1-10 and 26-30. 
  (3) There are way more non government institutions than government ones. Is this related to students' financial conditions? 
  (4) Our outcome variable, `adaptivity_level` is very imbalanced, so we need to stratify the samples when splitting the data.\

#### Age
Moving on, we generally believe that students who are older would perform better in online education, so is that really the case for our data? \

```{r}
p1 <- ggplot(adapt, aes(x=age, fill = adaptivity_level, color = adaptivity_level)) + 
           geom_bar(position = "dodge", alpha=0.5) + 
          theme(legend.position = "top")
p1 + labs(title = "Age vs. Adaptivity Level for Students", 
          x = "Age", y = "Number of Students")
```
\
Most students adapt to online education moderately, and only a tiny amount of them can adapt to it without any barriers (high adaptivity level). Students who are between 21-25 years old seem to adapt to online education the best as the number of students having a high adaptivity level is the highest. Probably institutions should also reconsider giving online education to students aged 6-10, 16-20, and 26-30 since the number of them having the low adaptivity level outnumbered the moderate and high adaptivity levels.\

#### Gender
So does gender have any effect on the adaptivity level?\

```{r}
adapt_gender <- adapt %>%
  group_by(age) %>%
  select(age, adaptivity_level, gender)

adapt_gender <- adapt_gender %>%
  transform(freq = ave(seq(nrow(adapt_gender)), age, FUN=length))
```

```{r}
adapt_gender %>%
  ggballoonplot(x = "age", y = "adaptivity_level", size = "freq", 
                fill = "freq", facet.by = "gender", 
                ggtheme = theme_bw()) + 
    scale_fill_viridis_c(option = "C")
```

```{r}
adapt %>%
  ggplot(aes(x=age, fill = adaptivity_level, color = adaptivity_level)) + 
  geom_bar() + 
  facet_grid(~gender)
```
\
It seems like the number of boys who have a high adaptivity level is more than that of the girls, so `gender` can be a relatively important predictor. However, since there are more boys than girls, we can't arrive to definite conclusion for this category.\

#### Education variables

Although online-education is a new form, at the end, it's still education, and vairables related to the quality of education that each student receives is important predictor for our project.\

So let's first take a look at whether `education_level` and `institution_type` is related to `adaptivity_level`.\
```{r}
adapt %>%
  ggplot(aes(x=education_level, group=institution_type)) + 
  geom_bar(aes(y=..prop.., fill = factor(..x..)), stat = 'count') +
  geom_text(stat = "count", aes(label=scales::percent(..prop..), y=..prop..), vjust=1.6, color="white", position = position_dodge(0.9), size=3.5)+
  labs(y = "Percent", fill = "adaptivity_level") +
  facet_grid(~institution_type) + 
  scale_y_continuous(labels = scales::percent) +
  theme_minimal()
```
\
As we see here, in government institutions, the higher the level of education, the better the adaptivity level. However, in non-government institutions, students who are in school mostly show a low adaptivity level, and in contrast to the increasing pattern from government institutions, students from non-government colleges also do not perform well in online-education. This indicates that institution type may be an important variable in our model.\

Then what about the effect of having an learning management system (`self_lms`) and a longer `class_duration` on students' `adaptivity_level`?\
```{r}
adapt %>%
  ggplot(aes(x=class_duration, fill = adaptivity_level, color = adaptivity_level)) + 
  geom_bar(position = "dodge", alpha=0.5) + 
  theme(legend.position = "top") + 
  labs(x = "Class Duration", y = "Number of Students")
```
\
The plot shows that students who don't have any class (`class_duration` = 0) have the highest rate of having a low adaptivity level. This is probably self-explanatory since students who do not adapt well to online education may choose to drop all the classes and take a gap semester. Students whose class duration is within 1-3 hours and within 3-6 hours adapt to online education moderately. Therefore, we can say that `class_duration`, especially for category 0, may be an important indicator for `adaptivity_level`.\

```{r}
adapt %>%
  ggplot(aes(x=self_lms, fill = adaptivity_level, color = adaptivity_level)) + 
  geom_bar(position = "dodge", alpha=0.5) + 
  theme(legend.position = "top")
```
\
Since the number of institutions that have an LMS and which haven't is significantly imbalanced, we can't really see the pattern here.\

#### Financial Condition

One argument we heard for online education is that students who are from families that are not very well-off tend to have a lower adaptivity level, for when they are home, they tend to be distracted by family duties, and they might not have good tech equipment required for online education such as stable internet or reliable network\

So is financial condition related to the types of internet and network that students used?\

```{r}
adapt %>%
  ggplot(aes(x=financial_condition, fill= network_type, color = network_type)) + 
  geom_bar() +
  facet_grid(~internet_type)
```
\
We can see that students who are categorized as "poor" in `financial_condition` use more mobile data than Wifi, which is a less stable way when taking online classes. They also tend to rely on 3G `network_type`, which is worse and slower than "4G". Among the students from "mid" `financial_condition` who use mobile data, they also tend to have a 3G network compared to "mid" students who use Wifi. In comparison, the number of students who are "rich" and use Wifi is more than the number of "rich" students using mobile data.\   

Therefore, we've seen that `financial_condition` has an effect on `internet_type` and `network_type`. Then, let's see if `financial_condition` affects the `adaptivity_level`.\

```{r}
adapt %>%
  ggplot(aes(x=financial_condition, fill = adaptivity_level)) + 
  geom_bar(position = position_dodge()) + 
  geom_text(stat = 'count', aes(label=after_stat(count)), vjust=1.6, color="white", position = position_dodge(0.9), size=3.5)+
  theme_minimal()
```
\
As we can tell, students who are "poor" have a higher rate of having a low adaptivity level, students who are "mid" have a higher rate of having a moderate adaptivity level, and students who are "rich" have the highest rate of having a high adaptivity level. In fact, although the total number of "rich" students is the lowest among all students surveyed, this group yielded the highest number of students who have a high adaptivity level.\

### Conclusion
From all the visualizations of our data, we can see that `age` and `financial_condition` are the two strongest predictor variables among the variables we tested. Between them, `financial_condition` is also related to `internet_type` and `network_type`. Therefore, in the later model building stage, we need to consider the interaction effect between these variables.\ 

Also, since all the variables are categorical, it's harder to finish the EDA compared to numeric variables. Thus, the forms of graphs might be onefold.\ 

## Model Building
Now it's time for building our models!\

### Splitting and folding the data
Since we have known from the summary table that the outcome variable `adaptivity_level` is imbalanced, we should stratify the samples and perform k-fold cross validation on the training set.\
```{r}
set.seed(1029)
adapt_split <- initial_split(adapt, prop = 0.7, strata = adaptivity_level)
adapt_train <- training(adapt_split)
adapt_test <- testing(adapt_split)
adapt_fold <- vfold_cv(adapt_train, v=10, strata = adaptivity_level)
```

### Recipe Building
Now let's build our recipe!\
```{r}
adapt_recipe <- recipe(adaptivity_level ~ gender + age + education_level + institution_type + it_student + location + load_shedding + financial_condition + internet_type + network_type + class_duration + self_lms + device, adapt_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_center(all_predictors()) %>%
  step_center(all_predictors())
```
In order to save time on knitting the document, we need to save the results.\
```{r}
save(adapt_fold, adapt_recipe, adapt_train, adapt_test, file = "model_setup.rda")
```

### Models
Since this project is a multiclass classification problem, we cannot use binary classification models, where are Logistic Regression, Linear Discriminant Analysis, and Quadratic Discriminant Analysis, to fit the data. Thus, the models we'll be trying out are Decision Tree, Random Forest, Boost, and KNN models. For these four models, we need to set up the tuning grid, tunning the model, and selecting the best-performing model.\

To save the space and time when knitting the report, all models were built in the R script. So here in the report, we'll only analyze the results of each model.\

#### Model results
First, let's plot out the results of each model/\

Decision Tree Model:\
```{r}
load("dt_tune_res.rda")
autoplot(dt_tune_res)
```
\
Decision tree model achieved an roc-auc close to 0.9 when the cost-complexity parameter is less than 0.01, which shows that it performs better with a smaller complexity penalty.\

Random Forest Tree model:\
```{r}
load("rf_tune_res.rda")
autoplot(rf_tune_res)
```
\
In random forest tree models, the greater the number of trees and randomly selected predictors, the better the performance of the model is. However, the performance seems to drop with an increasing number of minimal node size. As we can see, starting from minimal node size 7, the best roc-auc starts dropping when the minimal node size increases.\

Boosted Tree model:\
```{r}
load("boost_tune_res.rda")
autoplot(boost_tune_res)
```
\
The boost tree models clearly indicate that tuning the number of trees too high will lead to overfitting, which results in a lower roc-auc. When having a small minimal node size, the roc-auc is close to 1, meaning that it has a great measure of separability. \

K-Nearest Neighbor model:\
```{r}
load("knn_tune_res.rda")
autoplot(knn_tune_res)
```
\
The performance of the KNN model achieves the best roc-auc with 5 nearest neighbors, and the more neighbors it has, the less the roc-auc is.\

#### Selecting the best model
Now let's take a look at which parameters have the best result in each model and use it to finalize the workflow.\

Decision Tree model:\
```{r}
dt_result <- collect_metrics(dt_tune_res) %>%
  dplyr::arrange(desc(mean))
dt_result
```

```{r}
load("decision_tree_wf.rda")
set.seed(1029)
dt_best_mod <- select_best(dt_tune_res, metric = "roc_auc")
dt_tree_final <- finalize_workflow(decision_tree_wf, dt_best_mod)
dt_tree_final_fit <- fit(dt_tree_final, data = adapt_train) %>%
  extract_fit_engine() %>%
  rpart.plot()
```
\
The best performance model in our decision tree models achieves an roc_auc of 0.864.\

Random Forest Tree model:\
```{r}
rf_result <- collect_metrics(rf_tune_res) %>%
  dplyr::arrange(desc(mean))
rf_result
```
\
The best performance model in our random forest tree models achieves an roc-auc of 0.963, much better than the decision tree model.\
```{r}
load("rf_wf.rda")
set.seed(1029)
rf_best_mod <- select_best(rf_tune_res, metric = "roc_auc")
rf_final <- finalize_workflow(rf_wf, rf_best_mod)
rf_final_fit <- fit(rf_final, data = adapt_train)
rf_final_fit %>%
  pull_workflow_fit() %>%
  vip()
```
\
From the vip plot, we can see that the most important predictor is `institution_type`, which corroborated our earlier assumption in the EDA section. `gender`, especially whether a student is a girl, is also a vital predictor here, and we have also tested this out in the EDA. The top seven important predicators have all been discussed in the exploratory data analysis part, which indicates that the data analysis we did earlier is pretty useful!\

Boosted Tree model:\
```{r}
bt_result <- collect_metrics(boost_tune_res) %>%
  dplyr::arrange(desc(mean))
bt_result
```
The best performance model in our boosted tree models achieves an roc_auc pf 0.97, which is better than the best models of decision tree and random forest.\

```{r}
load("boost_wf.rda")
set.seed(1029)
boost_best_mod <- select_best(boost_tune_res, metric = "roc_auc")
boost_final_wf <- finalize_workflow(boost_wf, boost_best_mod)
boost_final_fit <- fit(boost_final_wf, data = adapt_train)
```

KNN model:\
```{r}
knn_result <- collect_metrics(knn_tune_res) %>%
  dplyr::arrange(desc(mean))
knn_result
```
The best performance model in the KNN models achieves the roc-auc of 0.925, which is better than the decision tree but worse than random forest and boosted tree.\
```{r}
load("knn_wf.rda")
set.seed(1029)
knn_best_mod <- select_best(knn_tune_res, metric = "roc_auc")
knn_final_wf <- finalize_workflow(knn_wf, knn_best_mod)
knn_final_fit <- fit(knn_final_wf, data = adapt_train)
```

#### Model Comparision
After selecting the best parameters out of each model, let's now compare them and choose the best one out of them to be our final model!\

```{r}
df_res = rbind(dt_result[1, c("mean", "std_err")], rf_result[1, c("mean", "std_err")], 
               bt_result[1, c("mean", "std_err")], knn_result[1, c("mean", "std_err")])
models <- c("Decision Tree", "Random Forest", "Boosted Tree", "KNN")
df_res = tibble(Models = models, df_res)
df_res %>%
  dplyr::arrange(desc(mean))
```
We can see from the comparison table that our boosted tree model achieves the highest roc_auc and the lowest standard error! So boosted tree model is the final model that we'll fit to our testing set!\
Let's try visualizing the result!\
```{r}
adapt_result <- ggplot(df_res, aes(x = Models, y = mean)) + 
  geom_segment(aes(x = Models, xend = Models, y = min(mean), yend = max(mean)), 
               linetype = "dashed", size=0.5) +
  geom_point(fill = "#FB4F14", col = "#FB4F14", size=10) +
  labs(title = "Performance of Our Models") + 
  theme_minimal() + 
  coord_flip()
adapt_result
```

### Fitting the final model on the testing set
We've now picked the best performance model of them all, so let's fit it on the testing set.\

#### ROC_AUC
```{r}
adapt_predicted <- augment(boost_final_fit, new_data = adapt_test) %>%
  select(adaptivity_level, starts_with(".pred")) %>%
  roc_auc(adaptivity_level, .pred_Low | .pred_Moderate | .pred_High)
adapt_predicted
```
Nice! The roc_auc of the model on the testing set is 0.958, which is an excellent performance!\

#### ROC Curve
Let's take a look at the roc plot as well.\

```{r}
adapt_roc_plot <- augment(boost_final_fit, new_data = adapt_test) %>%
  roc_curve(adaptivity_level, .pred_Low | .pred_Moderate | .pred_High) %>%
  autoplot()
adapt_roc_plot
```
\
Our roc curve follows a nearly perfect trajectory that is up and to the left.\

#### Confusion matrix
What about a confusion matrix?\
```{r}
adapt_conf_mat <- augment(boost_final_fit, new_data = adapt_test) %>%
  conf_mat(adaptivity_level, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
adapt_conf_mat
```
\
As we can see from the confusion matrix, the model is best at predicting the moderate `adaptivity_level`. This is because most students have a moderate `adaptivity_level`, so the model has more data of this category to train with. Except for the moderate level, it still did a pretty good job with low adaptivity. In comparison, it did not do very well with predicting high adaptivity, since the number of students who have a high `adaptivity_level` is the least.\

#### Example Predictions
We've seen from the metrics that our model performs very good on our testing set, but isn't it going to be more direct if we can test it out with specific student examples to see whether the prediction is correct?\

A student with low `adaptivity_level`:\
```{r}
adapt_eg1 <- data.frame(
  gender = "Girl", 
  age = "11-15",
  education_level = "School", 
  institution_type = "Government", 
  it_student = "No", 
  location = "Yes", 
  load_shedding = "Low", 
  financial_condition = "Mid", 
  internet_type = "Wifi", 
  network_type = "3G", 
  class_duration = "1-3", 
  self_lms = "No", 
  device = "Mobile"
)
predict(boost_final_fit, adapt_eg1, type = "class")
```
The prediction is correct for a low `adaptivity_level`!\

A student with moderate `adaptivity_level`:\
```{r}
adapt_eg2 <- data.frame(
  gender = "Boy", 
  age = "21-25",
  education_level = "University", 
  institution_type = "Non Government", 
  it_student = "No", 
  location = "Yes", 
  load_shedding = "Low", 
  financial_condition = "Mid", 
  internet_type = "Wifi", 
  network_type = "4G", 
  class_duration = "3-6", 
  self_lms = "No", 
  device = "Mobile"
)
predict(boost_final_fit, adapt_eg2, type = "class")
```
The prediction is again correct for a moderate `adaptivity_level`!\

A student with high `adaptivity_level`:\
```{r}
adapt_eg3 <- data.frame(
  gender = "Boy", 
  age = "21-25",
  education_level = "University", 
  institution_type = "Non Government", 
  it_student = "Yes", 
  location = "No", 
  load_shedding = "High", 
  financial_condition = "Mid", 
  internet_type = "Mobile Data", 
  network_type = "3G", 
  class_duration = "3-6", 
  self_lms = "Yes", 
  device = "Mobile"
)
predict(boost_final_fit, adapt_eg3, type = "class")
```
Congratulations! Our model has correctly predicted the `adaptivity_level` for all three examples!\

## Conclusion
This project has allowed me to explore machine learning from scratch: choosing a data set, analyzing its patterns, building models and selecting the best performance model based on various metrics, and finally putting it in use to predict the actual outcome.\

In my opinion, the hardest part of my project is doing the exploratory data analysis. Since all of my variables are categorical, the forms of graphs, especially plots that can show multivariate visualization of the relationship(s) between the outcome and select predictors, which I can use to visualize the data and analyze any correlation patterns are very limited. But in the end, I still managed to use bar plots to display the results I needed.\

Looking at the models, the best performance model is my boosted tree model, and the worst one is my decision tree model. This is because gradient boosting gives a prediction model in the form of an ensemble of weak decision tree models, and in this case, when a decision tree is a weak learner, the boosted tree usually outperforms the random forest.\

Overall, this project provided me with a great opportunity to put what I've learned in class into real use and help me practice my machine-learning techniques. This class has opened a gate for me to enter the world of machine learning and, further, deep learning.\